import torch
import time
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple
import pandas as pd

class PoolingBenchmark:
    def __init__(self, model_path: str):
        """åˆå§‹åŒ–åŸºå‡†æµ‹è¯•"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(model_path, torch_dtype=torch.float16)
        self.model.eval()
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def mean_pooling(self, token_embeddings: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """Mean poolingå®ç°"""
        # åº”ç”¨attention mask
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        masked_embeddings = token_embeddings * input_mask_expanded
        
        # è®¡ç®—å¹³å‡å€¼
        sum_embeddings = torch.sum(masked_embeddings, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        mean_embeddings = sum_embeddings / sum_mask
        
        # L2å½’ä¸€åŒ–
        return mean_embeddings / mean_embeddings.norm(dim=1, keepdim=True)
    
    def last_token_pooling(self, token_embeddings: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """Last token poolingå®ç°"""
        # æ‰¾åˆ°æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªæœ‰æ•ˆtokenä½ç½®
        seq_lengths = attention_mask.sum(dim=1) - 1
        batch_size = token_embeddings.shape[0]
        
        # æå–æœ€åä¸€ä¸ªtokençš„embedding
        last_embeddings = token_embeddings[torch.arange(batch_size), seq_lengths]
        
        # L2å½’ä¸€åŒ–
        return last_embeddings / last_embeddings.norm(dim=1, keepdim=True)
    
    def get_embeddings(self, texts: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–token embeddings"""
        # åˆ†è¯
        encoded = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            token_embeddings = outputs.last_hidden_state
        
        return token_embeddings, attention_mask
    
    def speed_benchmark(self, texts: List[str], num_runs: int = 100) -> Dict[str, float]:
        """é€Ÿåº¦åŸºå‡†æµ‹è¯•"""
        print(f"\n{'='*20} é€Ÿåº¦æµ‹è¯• {'='*20}")
        print(f"æµ‹è¯•æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"é‡å¤æ¬¡æ•°: {num_runs}")
        
        results = {}
        
        # é¢„çƒ­
        print("é¢„çƒ­ä¸­...")
        token_embeddings, attention_mask = self.get_embeddings(texts[:5])
        _ = self.mean_pooling(token_embeddings, attention_mask)
        _ = self.last_token_pooling(token_embeddings, attention_mask)
        
        # è·å–token embeddingsï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼Œæ’é™¤æ¨¡å‹æ¨ç†æ—¶é—´ï¼‰
        print("è·å–token embeddings...")
        start_time = time.time()
        token_embeddings, attention_mask = self.get_embeddings(texts)
        model_time = time.time() - start_time
        print(f"æ¨¡å‹æ¨ç†æ—¶é—´: {model_time:.4f}ç§’")
        
        # æµ‹è¯•Mean Pooling
        print("æµ‹è¯•Mean Pooling...")
        times = []
        for _ in range(num_runs):
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()
            _ = self.mean_pooling(token_embeddings, attention_mask)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            times.append(time.time() - start_time)
        
        mean_pooling_time = np.mean(times[10:])  # æ’é™¤å‰10æ¬¡çš„é¢„çƒ­
        results['mean_pooling'] = mean_pooling_time
        
        # æµ‹è¯•Last Token Pooling
        print("æµ‹è¯•Last Token Pooling...")
        times = []
        for _ in range(num_runs):
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()
            _ = self.last_token_pooling(token_embeddings, attention_mask)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            times.append(time.time() - start_time)
        
        last_token_time = np.mean(times[10:])  # æ’é™¤å‰10æ¬¡çš„é¢„çƒ­
        results['last_token'] = last_token_time
        
        # è¾“å‡ºç»“æœ
        print(f"\né€Ÿåº¦æµ‹è¯•ç»“æœ:")
        print(f"Mean Pooling:     {mean_pooling_time*1000:.3f} ms")
        print(f"Last Token:       {last_token_time*1000:.3f} ms")
        print(f"é€Ÿåº¦æå‡:          {mean_pooling_time/last_token_time:.2f}x")
        
        return results
    
    def semantic_benchmark(self) -> Dict[str, Dict]:
        """è¯­ä¹‰è´¨é‡åŸºå‡†æµ‹è¯•"""
        print(f"\n{'='*20} è¯­ä¹‰æµ‹è¯• {'='*20}")
        
        # æµ‹è¯•æ•°æ®é›†
        test_datasets = {
            "ç›¸ä¼¼å¥å¯¹": [
                ("The cat is on the mat", "A cat sits on the mat"),
                ("I love programming", "Programming is my passion"),
                ("The weather is nice", "It's a beautiful day"),
                ("Machine learning is powerful", "ML has great capabilities"),
                ("Hello world", "Hi there, world")
            ],
            "ä¸ç›¸ä¼¼å¥å¯¹": [
                ("The cat is on the mat", "I hate vegetables"),
                ("I love programming", "The ocean is deep"),
                ("The weather is nice", "Mathematics is difficult"),
                ("Machine learning is powerful", "I need to buy groceries"),
                ("Hello world", "Quantum physics is complex")
            ],
            "é•¿çŸ­å¥å¯¹æ¯”": [
                ("Hi", "Hello there, how are you doing today? I hope everything is going well."),
                ("Yes", "Absolutely, I completely agree with your assessment and think it's correct."),
                ("Good", "That's excellent news and I'm very happy to hear about this development."),
                ("Maybe", "I'm not entirely certain about this, but it might be possible under circumstances."),
                ("Thanks", "Thank you very much for your help, I really appreciate your time and effort.")
            ]
        }
        
        results = {}
        
        for dataset_name, pairs in test_datasets.items():
            print(f"\næµ‹è¯•æ•°æ®é›†: {dataset_name}")
            
            # æå–æ‰€æœ‰å¥å­
            all_sentences = []
            for sent1, sent2 in pairs:
                all_sentences.extend([sent1, sent2])
            
            # è·å–embeddings
            token_embeddings, attention_mask = self.get_embeddings(all_sentences)
            
            # è®¡ç®—ä¸åŒæ± åŒ–ç­–ç•¥çš„embeddings
            mean_embeddings = self.mean_pooling(token_embeddings, attention_mask)
            last_embeddings = self.last_token_pooling(token_embeddings, attention_mask)
            
            # è®¡ç®—å¥å­å¯¹ç›¸ä¼¼åº¦
            mean_similarities = []
            last_similarities = []
            
            for i in range(0, len(all_sentences), 2):
                # Mean poolingç›¸ä¼¼åº¦
                mean_sim = torch.cosine_similarity(
                    mean_embeddings[i:i+1], mean_embeddings[i+1:i+2], dim=1
                ).item()
                mean_similarities.append(mean_sim)
                
                # Last tokenç›¸ä¼¼åº¦
                last_sim = torch.cosine_similarity(
                    last_embeddings[i:i+1], last_embeddings[i+1:i+2], dim=1
                ).item()
                last_similarities.append(last_sim)
            
            # ç»Ÿè®¡ç»“æœ
            results[dataset_name] = {
                'mean_pooling': {
                    'similarities': mean_similarities,
                    'avg': np.mean(mean_similarities),
                    'std': np.std(mean_similarities)
                },
                'last_token': {
                    'similarities': last_similarities,
                    'avg': np.mean(last_similarities),
                    'std': np.std(last_similarities)
                }
            }
            
            # è¾“å‡ºè¯¦ç»†ç»“æœ
            print(f"Mean Pooling - å¹³å‡ç›¸ä¼¼åº¦: {np.mean(mean_similarities):.4f} (Â±{np.std(mean_similarities):.4f})")
            print(f"Last Token   - å¹³å‡ç›¸ä¼¼åº¦: {np.mean(last_similarities):.4f} (Â±{np.std(last_similarities):.4f})")
            
            # æ˜¾ç¤ºå…·ä½“ä¾‹å­
            for i, (sent1, sent2) in enumerate(pairs):
                print(f"  ä¾‹å­ {i+1}: {mean_similarities[i]:.3f} vs {last_similarities[i]:.3f}")
                print(f"    å¥å­1: {sent1}")
                print(f"    å¥å­2: {sent2}")
        
        return results
    
    def memory_usage_test(self, batch_sizes: List[int]) -> Dict[str, List[float]]:
        """å†…å­˜ä½¿ç”¨æµ‹è¯•"""
        print(f"\n{'='*20} å†…å­˜ä½¿ç”¨æµ‹è¯• {'='*20}")
        
        results = {'batch_sizes': batch_sizes, 'mean_pooling': [], 'last_token': []}
        test_text = "This is a test sentence for memory usage benchmark."
        
        for batch_size in batch_sizes:
            print(f"æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
            texts = [test_text] * batch_size
            token_embeddings, attention_mask = self.get_embeddings(texts)
            
            # æµ‹è¯•Mean Poolingå†…å­˜ä½¿ç”¨
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
                _ = self.mean_pooling(token_embeddings, attention_mask)
                mean_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            else:
                mean_memory = 0  # CPUå†…å­˜æµ‹è¯•è¾ƒå¤æ‚ï¼Œè¿™é‡Œç®€åŒ–
            
            # æµ‹è¯•Last Tokenå†…å­˜ä½¿ç”¨
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
                _ = self.last_token_pooling(token_embeddings, attention_mask)
                last_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            else:
                last_memory = 0
            
            results['mean_pooling'].append(mean_memory)
            results['last_token'].append(last_memory)
            
            print(f"  Mean Pooling: {mean_memory:.2f} MB")
            print(f"  Last Token:   {last_memory:.2f} MB")
        
        return results
    
    def comprehensive_report(self, speed_results: Dict, semantic_results: Dict, memory_results: Dict):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print(f"\n{'='*50}")
        print("ç»¼åˆå¯¹æ¯”æŠ¥å‘Š")
        print(f"{'='*50}")
        
        print("\nğŸš€ é€Ÿåº¦å¯¹æ¯”:")
        mean_time = speed_results['mean_pooling'] * 1000
        last_time = speed_results['last_token'] * 1000
        speedup = speed_results['mean_pooling'] / speed_results['last_token']
        
        print(f"  Mean Pooling: {mean_time:.3f} ms")
        print(f"  Last Token:   {last_time:.3f} ms")
        print(f"  Last Token æ¯” Mean Pooling å¿« {speedup:.2f} å€")
        
        print("\nğŸ¯ è¯­ä¹‰è´¨é‡å¯¹æ¯”:")
        for dataset, results in semantic_results.items():
            mean_avg = results['mean_pooling']['avg']
            last_avg = results['last_token']['avg']
            print(f"  {dataset}:")
            print(f"    Mean Pooling: {mean_avg:.4f}")
            print(f"    Last Token:   {last_avg:.4f}")
            print(f"    å·®å¼‚: {abs(mean_avg - last_avg):.4f}")
        
        if torch.cuda.is_available():
            print("\nğŸ’¾ å†…å­˜ä½¿ç”¨å¯¹æ¯”:")
            avg_mean_mem = np.mean(memory_results['mean_pooling'])
            avg_last_mem = np.mean(memory_results['last_token'])
            print(f"  Mean Pooling å¹³å‡: {avg_mean_mem:.2f} MB")
            print(f"  Last Token å¹³å‡:   {avg_last_mem:.2f} MB")
        
        print("\nğŸ“Š æ¨èä½¿ç”¨åœºæ™¯:")
        print("  Mean Pooling:")
        print("    âœ… éœ€è¦æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯")
        print("    âœ… æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—")
        print("    âœ… è¯­ä¹‰æœç´¢ä»»åŠ¡")
        print("    âœ… å¥å­èšç±»")
        
        print("  Last Token:")
        print("    âœ… æ¨ç†é€Ÿåº¦è¦æ±‚é«˜")
        print("    âœ… å¤§æ‰¹é‡å¤„ç†")
        print("    âœ… ä¸vLLMå…¼å®¹")
        print("    âœ… ç”Ÿæˆå¼ä»»åŠ¡")

def main():
    model_path = "/home/lz/repo/baselearn/llm/models/mmBERT-base"
    
    try:
        # åˆå§‹åŒ–æµ‹è¯•
        benchmark = PoolingBenchmark(model_path)
        
        # å‡†å¤‡æµ‹è¯•æ–‡æœ¬ï¼ˆä¸åŒé•¿åº¦ï¼‰
        test_texts = [
            "Hi",
            "Hello world!",
            "I am Justin, what about you?",
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is revolutionizing the way we process information.",
            "In the field of natural language processing, embeddings play a crucial role in understanding semantic meaning.",
            "Artificial intelligence has made tremendous progress in recent years, with applications spanning from computer vision to natural language understanding and beyond.",
        ] * 20  # åˆ›å»ºæ›´å¤§çš„æµ‹è¯•é›†
        
        # 1. é€Ÿåº¦åŸºå‡†æµ‹è¯•
        speed_results = benchmark.speed_benchmark(test_texts, num_runs=50)
        
        # 2. è¯­ä¹‰è´¨é‡æµ‹è¯•
        semantic_results = benchmark.semantic_benchmark()
        
        # 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
        memory_results = benchmark.memory_usage_test([1, 10, 50, 100])
        
        # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        benchmark.comprehensive_report(speed_results, semantic_results, memory_results)
        
    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿:")
        print("1. æ¨¡å‹è·¯å¾„æ­£ç¡®")
        print("2. æœ‰è¶³å¤Ÿçš„GPUå†…å­˜")
        print("3. å·²å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…")

if __name__ == "__main__":
    main()